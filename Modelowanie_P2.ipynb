{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOGAtlaEnT4A8oIWz1mjpNE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dorotahilger/Modelowanie/blob/main/Modelowanie_P2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbdFc0W-Nplr",
        "outputId": "c4cdb383-cf23-451c-a83d-4463c8860810"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "[1,   100] loss: 1.998\n",
            "[1,   200] loss: 1.669\n",
            "[1,   300] loss: 1.557\n",
            "[1,   400] loss: 1.450\n",
            "[1,   500] loss: 1.382\n",
            "[1,   600] loss: 1.309\n",
            "[1,   700] loss: 1.266\n",
            "[2,   100] loss: 1.208\n",
            "[2,   200] loss: 1.159\n",
            "[2,   300] loss: 1.150\n",
            "[2,   400] loss: 1.100\n",
            "[2,   500] loss: 1.095\n",
            "[2,   600] loss: 1.075\n",
            "[2,   700] loss: 1.050\n",
            "[3,   100] loss: 0.982\n",
            "[3,   200] loss: 0.970\n",
            "[3,   300] loss: 0.979\n",
            "[3,   400] loss: 0.953\n",
            "[3,   500] loss: 0.942\n",
            "[3,   600] loss: 0.927\n",
            "[3,   700] loss: 0.929\n",
            "[4,   100] loss: 0.844\n",
            "[4,   200] loss: 0.854\n",
            "[4,   300] loss: 0.858\n",
            "[4,   400] loss: 0.839\n",
            "[4,   500] loss: 0.829\n",
            "[4,   600] loss: 0.837\n",
            "[4,   700] loss: 0.863\n",
            "[5,   100] loss: 0.732\n",
            "[5,   200] loss: 0.786\n",
            "[5,   300] loss: 0.770\n",
            "[5,   400] loss: 0.771\n",
            "[5,   500] loss: 0.787\n",
            "[5,   600] loss: 0.756\n",
            "[5,   700] loss: 0.757\n",
            "[6,   100] loss: 0.712\n",
            "[6,   200] loss: 0.699\n",
            "[6,   300] loss: 0.688\n",
            "[6,   400] loss: 0.696\n",
            "[6,   500] loss: 0.730\n",
            "[6,   600] loss: 0.713\n",
            "[6,   700] loss: 0.727\n",
            "[7,   100] loss: 0.650\n",
            "[7,   200] loss: 0.654\n",
            "[7,   300] loss: 0.642\n",
            "[7,   400] loss: 0.655\n",
            "[7,   500] loss: 0.647\n",
            "[7,   600] loss: 0.666\n",
            "[7,   700] loss: 0.673\n",
            "[8,   100] loss: 0.582\n",
            "[8,   200] loss: 0.591\n",
            "[8,   300] loss: 0.593\n",
            "[8,   400] loss: 0.610\n",
            "[8,   500] loss: 0.622\n",
            "[8,   600] loss: 0.629\n",
            "[8,   700] loss: 0.631\n",
            "[9,   100] loss: 0.528\n",
            "[9,   200] loss: 0.541\n",
            "[9,   300] loss: 0.548\n",
            "[9,   400] loss: 0.575\n",
            "[9,   500] loss: 0.566\n",
            "[9,   600] loss: 0.578\n",
            "[9,   700] loss: 0.601\n",
            "[10,   100] loss: 0.508\n",
            "[10,   200] loss: 0.504\n",
            "[10,   300] loss: 0.512\n",
            "[10,   400] loss: 0.517\n",
            "[10,   500] loss: 0.519\n",
            "[10,   600] loss: 0.539\n",
            "[10,   700] loss: 0.541\n",
            "Accuracy on the test set: 71.63%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# 1. Przygotowanie danych\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),  # konwersja do tensoru\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]  # normalizacja\n",
        ")\n",
        "\n",
        "# Załadowanie zbioru treningowego i testowego CIFAR-10\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
        "\n",
        "# 2. Zbudowanie sieci konwolucyjnej\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        # Warstwy konwolucyjne\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "\n",
        "        # Warstwa w pełni połączona\n",
        "        # Początkowo, musimy obliczyć rozmiar wyjścia z warstw konwolucyjnych\n",
        "        self.fc1 = None  # Będzie inicjalizowane później\n",
        "\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "\n",
        "        # Funkcja aktywacji ReLU oraz max pooling\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "    def get_conv_output_shape(self, x):\n",
        "        # Przechodzimy przez wszystkie warstwy konwolucyjne i poolingowe\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        return x.size(1), x.size(2), x.size(3)  # Zwracamy: (kanały, wysokość, szerokość)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Obliczamy rozmiar po przejściu przez konwolucje\n",
        "        num_channels, height, width = self.get_conv_output_shape(x)\n",
        "\n",
        "        # Inicjalizujemy fc1 z obliczonymi wymiarami\n",
        "        if self.fc1 is None:\n",
        "            self.fc1 = nn.Linear(num_channels * height * width, 512)\n",
        "\n",
        "        x = self.pool(F.relu(self.conv1(x)))  # Konwolucja + ReLU + Pooling\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = x.view(x.size(0), -1)  # Flattening\n",
        "        x = F.relu(self.fc1(x))  # Warstwa w pełni połączona\n",
        "        x = self.fc2(x)  # Wyjście\n",
        "        return x\n",
        "\n",
        "# Inicjalizacja modelu\n",
        "model = CNN()\n",
        "\n",
        "# 3. Funkcja straty i optymalizator\n",
        "criterion = nn.CrossEntropyLoss()  # Strata dla klasyfikacji wieloklasowej\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Optymalizator Adam\n",
        "\n",
        "def train(model, trainloader, criterion, optimizer, num_epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()  # Zerowanie gradientów\n",
        "            outputs = model(inputs)  # Propagacja w przód\n",
        "            loss = criterion(outputs, labels)  # Obliczenie straty\n",
        "            loss.backward()  # Propagacja wsteczna\n",
        "            optimizer.step()  # Aktualizacja wag\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            if i % 100 == 99:  # Co 100 batch, wypisz stratę\n",
        "                print(f\"[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 100:.3f}\")\n",
        "                running_loss = 0.0\n",
        "\n",
        "def test(model, testloader):\n",
        "    model.eval()  # Ustawienie modelu w tryb testowy\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():  # Brak potrzeby obliczania gradientów podczas testowania\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)  # Uzyskanie predykcji\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Accuracy on the test set: {accuracy}%\")\n",
        "\n",
        "# Trening modelu\n",
        "train(model, trainloader, criterion, optimizer, num_epochs=10)\n",
        "\n",
        "# Testowanie modelu\n",
        "test(model, testloader)"
      ]
    }
  ]
}